{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperas.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danbernstein/parkingdirty/blob/master/hyperas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "q-b8sywacifv",
        "colab_type": "code",
        "outputId": "3d4f5d10-c133-408a-ef2c-05148cc916bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1516
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install pydrive\n",
        "! pip install hyperas\n",
        "! pip install hyperopt\n",
        "\n",
        "from __future__ import print_function\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "# load modules\n",
        "import os # for accessing project file structure\n",
        "import requests # for downloading and loading data from the internet\n",
        "import zipfile # for unzipping zip files \n",
        "\n",
        "import numpy.random \n",
        "from matplotlib import pyplot # for plotting results\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Running setup.py bdist_wheel for pydrive ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n",
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/54/72/5533b6bf9b47dc33685c3e62c391d6eab5785a648a5ffa841e240a3db3fe/hyperas-0.4.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.4)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.4.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.14.6)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.7.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (6.0.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.4.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.4.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.3.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.4.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.0.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hyperas) (4.3.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.2.4)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.1)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/e6/adb3be5576f5d27c6faa33f1e9fea8fe5dbd9351db12148de948507e352c/prompt_toolkit-2.0.7-py3-none-any.whl (338kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.4.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->hyperas) (1.1.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (2.5.3)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (40.6.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (4.6.0)\n",
            "Building wheels for collected packages: hyperas\n",
            "  Running setup.py bdist_wheel for hyperas ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/06/38/3f/27826f57fae60ef788ceb47e2c649590ab8af31f42075325d2\n",
            "Successfully built hyperas\n",
            "\u001b[31mipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.7 which is incompatible.\u001b[0m\n",
            "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
            "Installing collected packages: hyperas, prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 1.0.15\n",
            "    Uninstalling prompt-toolkit-1.0.15:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.15\n",
            "Successfully installed hyperas-0.4 prompt-toolkit-2.0.7\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.14.6)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.11.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.1.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.3.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NGppCGvdVFuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data():\n",
        "  \n",
        "  # download and read in data\n",
        "  zip_address = 'http://parkingdirty.com/BlockedBikeLaneTrainingFull.zip'\n",
        "  import numpy as np\n",
        "\n",
        "  import requests, zipfile, io\n",
        "  r = requests.get(zip_address)\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall() \n",
        "  \n",
        "  # DATA PROCESSING\n",
        "  \n",
        "  # append the label at front, and assign to object\n",
        "  imgs_blocked = list(map('blocked/{0}'.format, os.listdir('blocked')))\n",
        "  imgs_notblocked = list(map('notblocked/{0}'.format, os.listdir('notblocked')))\n",
        "\n",
        "  # shuffle data\n",
        "  numpy.random.shuffle(imgs_blocked)\n",
        "  numpy.random.shuffle(imgs_notblocked)\n",
        "\n",
        "  print(\"number of blocked images: \", len(imgs_blocked))\n",
        "  print(\"number of not blocked images: \", len(imgs_notblocked))\n",
        "  print(\"ratio of classes: \", len(imgs_blocked)/len(imgs_notblocked))\n",
        "  \n",
        "  # separate into training (contains training and validation), and test set\n",
        "  training_set = imgs_blocked[:int(round(0.8*len(imgs_blocked)))] + imgs_notblocked[:int(round(0.8*len(imgs_notblocked)))]\n",
        "  print(len(training_set))\n",
        "\n",
        "  test_set = imgs_blocked[int(round(0.8*len(imgs_blocked))):] + imgs_notblocked[int(round(0.8*len(imgs_notblocked))):]\n",
        "  print(len(test_set))\n",
        "\n",
        "  del imgs_blocked\n",
        "  del imgs_notblocked\n",
        "  \n",
        "  import cv2\n",
        "\n",
        "  training_arrays = [] # images\n",
        "  training_labels = [] # labels\n",
        "  test_arrays = []\n",
        "  test_labels = []\n",
        "            \n",
        "            \n",
        "  for i in training_set:\n",
        "    img_orig = cv2.imread(i)\n",
        "    img_arrays = cv2.resize(img_orig, dsize=(150, 150), interpolation=cv2.INTER_CUBIC)\n",
        "    training_arrays.append(img_arrays)\n",
        "    \n",
        "    if i.find(\"not\") is not -1:\n",
        "      val = 0\n",
        "      training_labels.append(val)\n",
        "    else:\n",
        "      val = 1\n",
        "      training_labels.append(val)\n",
        "      \n",
        "  for i in test_set:\n",
        "    img_orig = cv2.imread(i)\n",
        "    img_arrays = cv2.resize(img_orig, dsize=(150, 150), interpolation=cv2.INTER_CUBIC)\n",
        "    test_arrays.append(img_arrays)\n",
        "    \n",
        "    if i.find(\"not\") is not -1:\n",
        "      val = 0\n",
        "      test_labels.append(val)\n",
        "    else:\n",
        "      val = 1\n",
        "      test_labels.append(val)\n",
        "  \n",
        "  del training_set\n",
        "  del test_set\n",
        "  \n",
        "  X_train_array = np.array(training_arrays)\n",
        "  Y_train_labels = np.array(training_labels)\n",
        "\n",
        "  del training_arrays\n",
        "  del training_labels\n",
        "\n",
        "  X_test_array = np.array(test_arrays)\n",
        "  Y_test_labels = np.array(test_labels)\n",
        "\n",
        "  del test_arrays\n",
        "  del test_labels\n",
        "  \n",
        "  # separate training data into training and validation sets\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(X_train_array, Y_train_labels, test_size = 0.2, \n",
        "                                                      shuffle = True)\n",
        "  \n",
        "  # only resize the image arrays, \"resizing\" the labels would give 1.0 validation accuracy, which is not true \n",
        "  X_train //= 255\n",
        "  X_val //= 255\n",
        "  \n",
        "  print(len(Y_train))\n",
        "  print(len(Y_val))\n",
        "\n",
        "  \n",
        "  return X_train, Y_train, X_val, Y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9JxfOZXxbbtF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_hyp(X_train, Y_train, X_val, Y_val):\n",
        "    K.clear_session()\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  #Sigmoid function at the end because we have just two classes\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}},\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size={{choice([64, 128])}},\n",
        "              epochs=1,\n",
        "              verbose=2,\n",
        "              validation_data=(X_val, Y_val))\n",
        "   \n",
        "    score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
        "    print('Val accuracy:', acc)\n",
        "    print('optimizer:', model.optimizer)\n",
        "   \n",
        "    return {'loss': -acc, 'status': STATUS_OK}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qljAsequysn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Copy/download the file\n",
        "fid = drive.ListFile({'q':\"title = 'hyperas.ipynb'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('hyperas.ipynb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6CblR9nnVbZB",
        "colab_type": "code",
        "outputId": "ed7b7ee0-e2af-4c3c-8d46-6c7ca265ce73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6728
        }
      },
      "cell_type": "code",
      "source": [
        "best_run, best_model = optim.minimize(model=model_hyp,\n",
        "                                        data=data,\n",
        "                                        algo=tpe.suggest,\n",
        "                                        max_evals=30,\n",
        "                                        trials=Trials(),\n",
        "                                        notebook_name='hyperas') # has to match the name of the script"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "from __future__ import print_function\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import np_utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import os\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import requests\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import zipfile\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy.random\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from matplotlib import pyplot\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import requests, zipfile, io\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import cv2\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.auth import GoogleAuth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.drive import GoogleDrive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import auth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from oauth2client.client import GoogleCredentials\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
            "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
            "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "   1: \n",
            "   2: \n",
            "   3: # download and read in data\n",
            "   4: zip_address = 'http://parkingdirty.com/BlockedBikeLaneTrainingFull.zip'\n",
            "   5: import numpy as np\n",
            "   6: \n",
            "   7: import requests, zipfile, io\n",
            "   8: r = requests.get(zip_address)\n",
            "   9: z = zipfile.ZipFile(io.BytesIO(r.content))\n",
            "  10: z.extractall() \n",
            "  11: \n",
            "  12: # DATA PROCESSING\n",
            "  13: \n",
            "  14: # append the label at front, and assign to object\n",
            "  15: imgs_blocked = list(map('blocked/{0}'.format, os.listdir('blocked')))\n",
            "  16: imgs_notblocked = list(map('notblocked/{0}'.format, os.listdir('notblocked')))\n",
            "  17: \n",
            "  18: # shuffle data\n",
            "  19: numpy.random.shuffle(imgs_blocked)\n",
            "  20: numpy.random.shuffle(imgs_notblocked)\n",
            "  21: \n",
            "  22: print(\"number of blocked images: \", len(imgs_blocked))\n",
            "  23: print(\"number of not blocked images: \", len(imgs_notblocked))\n",
            "  24: print(\"ratio of classes: \", len(imgs_blocked)/len(imgs_notblocked))\n",
            "  25: \n",
            "  26: # separate into training (contains training and validation), and test set\n",
            "  27: training_set = imgs_blocked[:int(round(0.8*len(imgs_blocked)))] + imgs_notblocked[:int(round(0.8*len(imgs_notblocked)))]\n",
            "  28: print(len(training_set))\n",
            "  29: \n",
            "  30: test_set = imgs_blocked[int(round(0.8*len(imgs_blocked))):] + imgs_notblocked[int(round(0.8*len(imgs_notblocked))):]\n",
            "  31: print(len(test_set))\n",
            "  32: \n",
            "  33: del imgs_blocked\n",
            "  34: del imgs_notblocked\n",
            "  35: \n",
            "  36: import cv2\n",
            "  37: \n",
            "  38: training_arrays = [] # images\n",
            "  39: training_labels = [] # labels\n",
            "  40: test_arrays = []\n",
            "  41: test_labels = []\n",
            "  42:           \n",
            "  43:           \n",
            "  44: for i in training_set:\n",
            "  45:   img_orig = cv2.imread(i)\n",
            "  46:   img_arrays = cv2.resize(img_orig, dsize=(150, 150), interpolation=cv2.INTER_CUBIC)\n",
            "  47:   training_arrays.append(img_arrays)\n",
            "  48:   \n",
            "  49:   if i.find(\"not\") is not -1:\n",
            "  50:     val = 0\n",
            "  51:     training_labels.append(val)\n",
            "  52:   else:\n",
            "  53:     val = 1\n",
            "  54:     training_labels.append(val)\n",
            "  55:     \n",
            "  56: for i in test_set:\n",
            "  57:   img_orig = cv2.imread(i)\n",
            "  58:   img_arrays = cv2.resize(img_orig, dsize=(150, 150), interpolation=cv2.INTER_CUBIC)\n",
            "  59:   test_arrays.append(img_arrays)\n",
            "  60:   \n",
            "  61:   if i.find(\"not\") is not -1:\n",
            "  62:     val = 0\n",
            "  63:     test_labels.append(val)\n",
            "  64:   else:\n",
            "  65:     val = 1\n",
            "  66:     test_labels.append(val)\n",
            "  67: \n",
            "  68: del training_set\n",
            "  69: del test_set\n",
            "  70: \n",
            "  71: X_train_array = np.array(training_arrays)\n",
            "  72: Y_train_labels = np.array(training_labels)\n",
            "  73: \n",
            "  74: del training_arrays\n",
            "  75: del training_labels\n",
            "  76: \n",
            "  77: X_test_array = np.array(test_arrays)\n",
            "  78: Y_test_labels = np.array(test_labels)\n",
            "  79: \n",
            "  80: del test_arrays\n",
            "  81: del test_labels\n",
            "  82: \n",
            "  83: # separate training data into training and validation sets\n",
            "  84: \n",
            "  85: from sklearn.model_selection import train_test_split\n",
            "  86: \n",
            "  87: X_train, X_val, Y_train, Y_val = train_test_split(X_train_array, Y_train_labels, test_size = 0.2, \n",
            "  88:                                                     shuffle = True)\n",
            "  89: \n",
            "  90: # only resize the image arrays, \"resizing\" the labels would give 1.0 validation accuracy, which is not true \n",
            "  91: X_train //= 255\n",
            "  92: X_val //= 255\n",
            "  93: \n",
            "  94: print(len(Y_train))\n",
            "  95: print(len(Y_val))\n",
            "  96: \n",
            "  97: \n",
            "  98: \n",
            "  99: \n",
            " 100: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "  1: def keras_fmin_fnct(space):\n",
            "  2: \n",
            "  3:     K.clear_session()\n",
            "  4: \n",
            "  5:     model = Sequential()\n",
            "  6:     model.add(Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))\n",
            "  7:     model.add(Conv2D(32, (3, 3), activation='relu'))\n",
            "  8:     model.add(Dropout(space['Dropout']))\n",
            "  9: \n",
            " 10:     model.add(Flatten())\n",
            " 11:     model.add(Dense(128, activation='relu'))\n",
            " 12:     model.add(Dense(1, activation='sigmoid'))  #Sigmoid function at the end because we have just two classes\n",
            " 13: \n",
            " 14:     model.compile(loss='binary_crossentropy',\n",
            " 15:                   optimizer=space['optimizer'],\n",
            " 16:                   metrics=['accuracy'])\n",
            " 17: \n",
            " 18:     model.fit(X_train, Y_train,\n",
            " 19:               batch_size=space['batch_size'],\n",
            " 20:               epochs=1,\n",
            " 21:               verbose=2,\n",
            " 22:               validation_data=(X_val, Y_val))\n",
            " 23:    \n",
            " 24:     score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
            " 25:     print('Val accuracy:', acc)\n",
            " 26:     print('optimizer:', model.optimizer)\n",
            " 27:    \n",
            " 28:     return {'loss': -acc, 'status': STATUS_OK}\n",
            " 29: \n",
            "number of blocked images:  4131\n",
            "number of not blocked images:  3542\n",
            "ratio of classes:  1.166290231507623\n",
            "6139\n",
            "1534\n",
            "4911\n",
            "1228\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 24s - loss: 0.6790 - acc: 0.6066 - val_loss: 0.6713 - val_acc: 0.6148\n",
            "Val accuracy: 0.6148208470996894\n",
            "optimizer: <keras.optimizers.SGD object at 0x7fdc2ef73eb8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 24s - loss: 0.6766 - acc: 0.6074 - val_loss: 0.6483 - val_acc: 0.6360\n",
            "Val accuracy: 0.6359934851478676\n",
            "optimizer: <keras.optimizers.RMSprop object at 0x7fdc282c3f98>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6541 - acc: 0.6225 - val_loss: 0.6199 - val_acc: 0.6441\n",
            "Val accuracy: 0.6441368080117416\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc25566ef0>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 20s - loss: 0.6848 - acc: 0.6044 - val_loss: 0.6409 - val_acc: 0.6270\n",
            "Val accuracy: 0.6270358308130445\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2ef2f240>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 19s - loss: 8.2589 - acc: 0.4702 - val_loss: 8.8860 - val_acc: 0.4487\n",
            "Val accuracy: 0.44869706820975686\n",
            "optimizer: <keras.optimizers.RMSprop object at 0x7fdc2806af98>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 17s - loss: 0.6784 - acc: 0.6082 - val_loss: 0.6705 - val_acc: 0.6148\n",
            "Val accuracy: 0.6148208470996894\n",
            "optimizer: <keras.optimizers.SGD object at 0x7fdc2e545828>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 19s - loss: 0.7582 - acc: 0.5982 - val_loss: 0.6595 - val_acc: 0.6164\n",
            "Val accuracy: 0.6164495114006515\n",
            "optimizer: <keras.optimizers.RMSprop object at 0x7fdc27f8ae80>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 17s - loss: 0.6783 - acc: 0.6078 - val_loss: 0.6698 - val_acc: 0.6148\n",
            "Val accuracy: 0.6148208470996894\n",
            "optimizer: <keras.optimizers.SGD object at 0x7fdc252c4c50>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 19s - loss: 0.7026 - acc: 0.5980 - val_loss: 0.6973 - val_acc: 0.6148\n",
            "Val accuracy: 0.6148208470996894\n",
            "optimizer: <keras.optimizers.RMSprop object at 0x7fdc2e68e4a8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6648 - acc: 0.6284 - val_loss: 0.6100 - val_acc: 0.6702\n",
            "Val accuracy: 0.6701954399335657\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2e5b2ba8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 21s - loss: 0.6748 - acc: 0.5899 - val_loss: 0.6350 - val_acc: 0.6409\n",
            "Val accuracy: 0.6408794790215135\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2ef03160>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6573 - acc: 0.6188 - val_loss: 0.6295 - val_acc: 0.6498\n",
            "Val accuracy: 0.6498371333563366\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc252d9f98>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 21s - loss: 0.6793 - acc: 0.5846 - val_loss: 0.6441 - val_acc: 0.6368\n",
            "Val accuracy: 0.6368078173954246\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2e68e6d8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 19s - loss: 0.7706 - acc: 0.5771 - val_loss: 0.6700 - val_acc: 0.6376\n",
            "Val accuracy: 0.6376221496429816\n",
            "optimizer: <keras.optimizers.RMSprop object at 0x7fdc27e6deb8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 21s - loss: 0.6753 - acc: 0.6101 - val_loss: 0.6304 - val_acc: 0.6498\n",
            "Val accuracy: 0.6498371337446406\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc24e1aac8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 21s - loss: 0.6952 - acc: 0.5942 - val_loss: 0.6358 - val_acc: 0.6409\n",
            "Val accuracy: 0.6408794790215135\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc28146358>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 21s - loss: 0.6710 - acc: 0.5993 - val_loss: 0.6384 - val_acc: 0.6336\n",
            "Val accuracy: 0.6335504884051966\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2eead828>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 19s - loss: 0.7351 - acc: 0.5811 - val_loss: 0.6504 - val_acc: 0.6433\n",
            "Val accuracy: 0.6433224753758806\n",
            "optimizer: <keras.optimizers.RMSprop object at 0x7fdc280aaeb8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 19s - loss: 0.7879 - acc: 0.5879 - val_loss: 0.6468 - val_acc: 0.6466\n",
            "Val accuracy: 0.6465798047544126\n",
            "optimizer: <keras.optimizers.RMSprop object at 0x7fdc38f97e80>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6576 - acc: 0.6276 - val_loss: 0.6302 - val_acc: 0.6376\n",
            "Val accuracy: 0.6376221500312855\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2e40dd68>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6387 - acc: 0.6487 - val_loss: 0.6001 - val_acc: 0.6669\n",
            "Val accuracy: 0.6669381105550337\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc27f72668>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6597 - acc: 0.6341 - val_loss: 0.6113 - val_acc: 0.6735\n",
            "Val accuracy: 0.6734527687296417\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2e465ac8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6535 - acc: 0.6363 - val_loss: 0.6021 - val_acc: 0.6735\n",
            "Val accuracy: 0.6734527689237936\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc25260390>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 20s - loss: 0.6747 - acc: 0.6082 - val_loss: 0.6906 - val_acc: 0.5513\n",
            "Val accuracy: 0.5513029314990152\n",
            "optimizer: <keras.optimizers.SGD object at 0x7fdc282c5ef0>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6898 - acc: 0.5936 - val_loss: 0.6818 - val_acc: 0.6148\n",
            "Val accuracy: 0.6148208470996894\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2e630dd8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6614 - acc: 0.6298 - val_loss: 0.6126 - val_acc: 0.6588\n",
            "Val accuracy: 0.6587947880794637\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2533ba90>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6781 - acc: 0.6190 - val_loss: 0.6229 - val_acc: 0.6507\n",
            "Val accuracy: 0.6506514659921976\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc2e4393c8>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 20s - loss: 0.6763 - acc: 0.6040 - val_loss: 0.6660 - val_acc: 0.6148\n",
            "Val accuracy: 0.6148208470996894\n",
            "optimizer: <keras.optimizers.SGD object at 0x7fdc27e0cef0>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 26s - loss: 0.6518 - acc: 0.6308 - val_loss: 0.6419 - val_acc: 0.6425\n",
            "Val accuracy: 0.6425081433224755\n",
            "optimizer: <keras.optimizers.Adam object at 0x7fdc38f0fda0>\n",
            "Train on 4911 samples, validate on 1228 samples\n",
            "Epoch 1/1\n",
            " - 20s - loss: 0.6751 - acc: 0.6094 - val_loss: 0.6650 - val_acc: 0.6148\n",
            "Val accuracy: 0.6148208470996894\n",
            "optimizer: <keras.optimizers.SGD object at 0x7fdc281bab70>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}